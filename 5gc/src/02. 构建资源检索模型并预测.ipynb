{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. 构建资源检索模型并预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.1.post300\n",
      "device: cuda\n",
      "python version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "print(f'torch version: {torch.__version__}')\n",
    "print(f'device: {torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")}')\n",
    "print(f'python version: {sys.version}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 构建GNN基础模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResourceGNN(\n",
      "  (node_embedding): Embedding(10, 64)\n",
      "  (conv1): GATConv(64, 64, heads=4)\n",
      "  (conv2): GATConv(256, 64, heads=1)\n",
      "  (batch_norm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ResourceGNN(nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_dim=64):\n",
    "        super(ResourceGNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # 节点嵌入层 - 使用nn.Embedding而不是直接处理\n",
    "        self.node_embedding = nn.Embedding(num_node_features, hidden_dim)\n",
    "        \n",
    "        # GNN层 - 修改输入输出维度\n",
    "        self.conv1 = GATConv(in_channels=hidden_dim, \n",
    "                            out_channels=hidden_dim, \n",
    "                            heads=4, \n",
    "                            concat=True,\n",
    "                            dropout=0.2)\n",
    "        \n",
    "        self.conv2 = GATConv(in_channels=hidden_dim*4,  # 因为第一层有4个头\n",
    "                            out_channels=hidden_dim,\n",
    "                            heads=1,\n",
    "                            concat=False,\n",
    "                            dropout=0.2)\n",
    "        \n",
    "        # 添加批标准化\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_dim * 4)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # 确保输入是在正确的设备上\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # 节点特征嵌入\n",
    "        x = self.node_embedding(x)  # [num_nodes, hidden_dim]\n",
    "        \n",
    "        # 第一层图注意力\n",
    "        x = self.conv1(x, edge_index)  # [num_nodes, hidden_dim * 4]\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # 第二层图注意力\n",
    "        x = self.conv2(x, edge_index)  # [num_nodes, hidden_dim]\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 打印模型结构\n",
    "print(ResourceGNN(num_node_features=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理模型\n",
    "\n",
    "class ResourceDataset(Dataset):\n",
    "    def __init__(self, graph_data, node_types):\n",
    "        super(ResourceDataset, self).__init__()\n",
    "        self.graph_data = graph_data\n",
    "        self.node_types = node_types\n",
    "        \n",
    "        # 编码节点类型\n",
    "        self.type_encoder = LabelEncoder()\n",
    "        self.type_encoder.fit(list(node_types))\n",
    "        \n",
    "        # 构建训练数据\n",
    "        self.data_list = self._prepare_data()\n",
    "        \n",
    "    def _prepare_data(self):\n",
    "        # 将NetworkX图转换为PyG数据\n",
    "        G = nx.Graph(self.graph_data)\n",
    "        \n",
    "        # 节点特征\n",
    "        nodes = list(G.nodes())\n",
    "        node_features = []\n",
    "        node_mapping = {node: idx for idx, node in enumerate(nodes)}\n",
    "        \n",
    "        for node in nodes:\n",
    "            node_type = G.nodes[node]['type']\n",
    "            node_features.append(self.type_encoder.transform([node_type])[0])\n",
    "            \n",
    "        # 边特征\n",
    "        edge_index = []\n",
    "        for src, dst in G.edges():\n",
    "            edge_index.append([node_mapping[src], node_mapping[dst]])\n",
    "            edge_index.append([node_mapping[dst], node_mapping[src]])  # 无向图\n",
    "            \n",
    "        # 转换为PyTorch张量\n",
    "        x = torch.tensor(node_features, dtype=torch.long)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        \n",
    "        # 创建PyG数据对象\n",
    "        data = Data(x=x, \n",
    "                   edge_index=edge_index, \n",
    "                   num_nodes=len(nodes))\n",
    "        \n",
    "        return [data]\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResourceSearchModelGNN:\n",
    "    def __init__(self, model_path=None):\n",
    "        # 加载图数据\n",
    "        self.graph_data = self._load_graph_data()\n",
    "        \n",
    "        # 获取节点类型\n",
    "        self.node_types = set()\n",
    "        for _, attr in self.graph_data.nodes(data=True):\n",
    "            self.node_types.add(attr['type'])\n",
    "        \n",
    "        # 创建数据集\n",
    "        self.dataset = ResourceDataset(self.graph_data, self.node_types)\n",
    "        \n",
    "        # 创建模型\n",
    "        self.model = ResourceGNN(\n",
    "            num_node_features=len(self.node_types),\n",
    "            num_edge_features=1,\n",
    "            hidden_dim=64\n",
    "        )\n",
    "        \n",
    "        # 加载预训练模型\n",
    "        if model_path:\n",
    "            self.model.load_state_dict(torch.load(model_path))\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "    def _load_graph_data(self):\n",
    "        \"\"\"加载图数据\"\"\"\n",
    "        with open('../datasets/results/cross_layer_mapping.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        # 构建NetworkX图\n",
    "        G = nx.Graph()\n",
    "        # ... 构建图的代码 ...\n",
    "        return G\n",
    "    \n",
    "    \n",
    "    def train(self, epochs=100, batch_size=32):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        self.model.train()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        loader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch in loader:\n",
    "                batch = batch.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 前向传播\n",
    "                out = self.model(batch.x, batch.edge_index)\n",
    "                \n",
    "                # 计算损失（这里使用示例损失函数）\n",
    "                loss = self._compute_loss(out, batch)\n",
    "                \n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(loader):.4f}')\n",
    "            \n",
    "            \n",
    "    def _compute_loss(self, pred, batch):\n",
    "            \"\"\"计算损失函数\"\"\"\n",
    "        # 这里实现具体的损失函数\n",
    "        # 可以根据任务需求设计不同的损失函数\n",
    "            pass\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"保存模型\"\"\"\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "    \n",
    "    def search(self, query_node: str, node_type: str):\n",
    "        \"\"\"使用训练好的模型进行资源检索\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 将查询节点转换为模型输入格式\n",
    "        node_idx = list(self.graph_data.nodes()).index(query_node)\n",
    "        data = self.dataset[0].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 获取节点嵌入\n",
    "            node_embeddings = self.model(data.x, data.edge_index)\n",
    "            query_embedding = node_embeddings[node_idx]\n",
    "            \n",
    "            # 计算与其他节点的相似度\n",
    "            similarities = F.cosine_similarity(\n",
    "                query_embedding.unsqueeze(0),\n",
    "                node_embeddings,\n",
    "                dim=1\n",
    "            )\n",
    "            \n",
    "            # 获取最相关的节点\n",
    "            top_k = 10\n",
    "            top_indices = similarities.topk(top_k).indices.cpu().numpy()\n",
    "            \n",
    "            # 构建子图\n",
    "            related_nodes = [list(self.graph_data.nodes())[i] for i in top_indices]\n",
    "            subgraph = self.graph_data.subgraph(related_nodes)\n",
    "            \n",
    "            # 构建结果字符串\n",
    "            result_str = self._build_result_string(query_node, related_nodes)\n",
    "            \n",
    "            return result_str, subgraph\n",
    "        \n",
    "    def _build_result_string(self, query_node, related_nodes):\n",
    "            \"\"\"构建结果字符串\"\"\"\n",
    "        # ... 实现结果字符串构建逻辑 ...\n",
    "            pass\n",
    "    \n",
    "    def visualize_subgraph(self, subgraph):\n",
    "        \"\"\"可视化子图\"\"\"\n",
    "        # ... 实现子图可视化逻辑 ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练代码修改\n",
    "def train_model(model, dataset, device, epochs=100):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for data in DataLoader(dataset, batch_size=1):  # 由于是单个图，batch_size=1\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            out = model(data)\n",
    "            \n",
    "            # 这里使用重构损失作为示例\n",
    "            # 可以根据具体任务修改损失函数\n",
    "            loss = F.mse_loss(out, torch.zeros_like(out))  # 示例损失\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1:03d}, Loss: {total_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 加载数据进行训练\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_data():\n",
    "    \"\"\"加载所有图数据并合并成一个完整的图\"\"\"\n",
    "    base_path = '../datasets/results/'\n",
    "    \n",
    "    # 加载三个JSON文件\n",
    "    with open(os.path.join(base_path, 'tenant_graph_dict.json'), 'r', encoding='utf-8') as f:\n",
    "        virtual_graph = json.load(f)\n",
    "    \n",
    "    with open(os.path.join(base_path, 'ha_graph_dict.json'), 'r', encoding='utf-8') as f:\n",
    "        physical_graph = json.load(f)\n",
    "        \n",
    "    with open(os.path.join(base_path, 'cross_layer_mapping.json'), 'r', encoding='utf-8') as f:\n",
    "        cross_layer = json.load(f)\n",
    "    \n",
    "    # 创建完整的NetworkX图\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # 添加虚拟资源图的节点和边\n",
    "    for node in virtual_graph['nodes']:\n",
    "        # 使用id作为节点标识符\n",
    "        node_id = node['id']\n",
    "        # 添加节点及其属性\n",
    "        G.add_node(node_id, \n",
    "                  type=node['type'],\n",
    "                  node_id=node['node_id'])\n",
    "    \n",
    "    # 添加虚拟资源图的边\n",
    "    for link in virtual_graph['links']:\n",
    "        G.add_edge(link['source'], \n",
    "                  link['target'], \n",
    "                  connection_type=link['connection_type'])\n",
    "    \n",
    "    # 添加物理资源图的节点和边\n",
    "    for node in physical_graph['nodes']:\n",
    "        node_id = node['id']\n",
    "        if node_id not in G:  # 避免重复添加已存在的节点\n",
    "            G.add_node(node_id,\n",
    "                      type=node['type'],\n",
    "                      node_id=node['node_id'])\n",
    "            \n",
    "    for link in physical_graph['links']:\n",
    "        G.add_edge(link['source'], \n",
    "                  link['target'],\n",
    "                  connection_type=link['connection_type'])\n",
    "    \n",
    "    # 添加跨层映射关系\n",
    "    for mapping in cross_layer['virtual_to_physical']:\n",
    "        vm = mapping['virtual_layer'].get('virtual_machine')\n",
    "        host = mapping['physical_layer'].get('host')\n",
    "        ne = mapping['virtual_layer'].get('network_element')\n",
    "        ha = mapping['physical_layer'].get('ha')\n",
    "        tru = mapping['physical_layer'].get('tru')\n",
    "        \n",
    "        # 添加虚拟机到主机的映射\n",
    "        if vm and host:\n",
    "            if vm not in G:\n",
    "                G.add_node(vm, type='virtual_machine')\n",
    "            if host not in G:\n",
    "                G.add_node(host, type='host')\n",
    "            G.add_edge(vm, host, connection_type='cross_layer')\n",
    "        \n",
    "        # 添加网元到HA的映射\n",
    "        if ne and ha:\n",
    "            if ne not in G:\n",
    "                G.add_node(ne, type='network_element')\n",
    "            if ha not in G:\n",
    "                G.add_node(ha, type='ha')\n",
    "            G.add_edge(ne, ha, connection_type='cross_layer')\n",
    "            \n",
    "        # 添加主机到TRU的映射\n",
    "        if host and tru:\n",
    "            if tru not in G:\n",
    "                G.add_node(tru, type='tru')\n",
    "            G.add_edge(host, tru, connection_type='physical')\n",
    "    \n",
    "    # 打印统计信息\n",
    "    print(\"\\nGraph summary:\")\n",
    "    print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "    \n",
    "    # 统计节点类型\n",
    "    node_types = {}\n",
    "    for _, attr in G.nodes(data=True):\n",
    "        node_type = attr.get('type')\n",
    "        if node_type:\n",
    "            node_types[node_type] = node_types.get(node_type, 0) + 1\n",
    "    \n",
    "    print(\"\\nNode types and counts:\")\n",
    "    for node_type, count in node_types.items():\n",
    "        print(f\"  {node_type}: {count}\")\n",
    "    \n",
    "    return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 创建PyG数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建PyG数据集\n",
    "class ResourceDataset(Dataset):\n",
    "    def __init__(self, graph_data):\n",
    "        super(ResourceDataset, self).__init__()\n",
    "        self.graph_data = graph_data\n",
    "        \n",
    "        # 获取所有节点类型\n",
    "        self.node_types = set()\n",
    "        for _, attr in graph_data.nodes(data=True):\n",
    "            self.node_types.add(attr['type'])\n",
    "        \n",
    "        # 编码节点类型\n",
    "        self.type_encoder = LabelEncoder()\n",
    "        self.type_encoder.fit(list(self.node_types))\n",
    "        \n",
    "        # 构建训练数据\n",
    "        self.data = self._prepare_data()\n",
    "        \n",
    "    def _prepare_data(self):\n",
    "        # 节点特征\n",
    "        nodes = list(self.graph_data.nodes())\n",
    "        node_features = []\n",
    "        node_mapping = {node: idx for idx, node in enumerate(nodes)}\n",
    "        \n",
    "        for node in nodes:\n",
    "            node_type = self.graph_data.nodes[node]['type']\n",
    "            type_id = self.type_encoder.transform([node_type])[0]\n",
    "            node_features.append(type_id)\n",
    "            \n",
    "        # 边特征\n",
    "        edge_index = []\n",
    "        edge_type = []\n",
    "        for src, dst, data in self.graph_data.edges(data=True):\n",
    "            edge_index.append([node_mapping[src], node_mapping[dst]])\n",
    "            edge_index.append([node_mapping[dst], node_mapping[src]])  # 无向图\n",
    "            # 边类型特征\n",
    "            conn_type = data.get('connection_type', 'default')\n",
    "            edge_type.extend([conn_type, conn_type])  # 双向边\n",
    "            \n",
    "        # 转换为PyTorch张量\n",
    "        x = torch.tensor(node_features, dtype=torch.long)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        \n",
    "        # 创建PyG数据对象\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            num_nodes=len(nodes),\n",
    "            node_mapping=node_mapping,\n",
    "            reverse_mapping={v: k for k, v in node_mapping.items()}\n",
    "        )\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def len(self):\n",
    "        return 1  # 只有一个图\n",
    "    \n",
    "    def get(self, idx):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResourceGNN(nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_dim=64):\n",
    "        super(ResourceGNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # 节点嵌入层\n",
    "        self.node_embedding = nn.Embedding(num_node_features, hidden_dim)\n",
    "        \n",
    "        # GNN层\n",
    "        self.conv1 = GATConv(hidden_dim, hidden_dim, heads=4, dropout=0.2)\n",
    "        self.conv2 = GATConv(hidden_dim*4, hidden_dim, heads=1, dropout=0.2)\n",
    "        \n",
    "        # 批标准化层\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_dim * 4)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # 节点特征嵌入\n",
    "        x = self.node_embedding(x)\n",
    "        \n",
    "        # 第一层图注意力\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # 第二层图注意力\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(node_embeddings, data):\n",
    "    \"\"\"\n",
    "    计算损失函数 - 这里使用节点分类任务作为示例\n",
    "    你可以根据具体需求修改这个函数\n",
    "    \"\"\"\n",
    "    # 示例：使用相似节点的embeddings应该相近作为损失\n",
    "    loss = 0\n",
    "    for i in range(len(data.edge_index[0])):\n",
    "        src_idx = data.edge_index[0][i]\n",
    "        dst_idx = data.edge_index[1][i]\n",
    "        \n",
    "        src_emb = node_embeddings[src_idx]\n",
    "        dst_emb = node_embeddings[dst_idx]\n",
    "        \n",
    "        # 计算相连节点的embedding相似度\n",
    "        similarity = F.cosine_similarity(src_emb.unsqueeze(0), dst_emb.unsqueeze(0))\n",
    "        loss += 1 - similarity  # 最大化相似度\n",
    "    \n",
    "    return loss / len(data.edge_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, device, epochs=100):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    \n",
    "    data = dataset[0].to(device)  # 获取图数据\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        node_embeddings = model(data)\n",
    "        \n",
    "        # 计算损失 - 这里使用节点分类任务作为示例\n",
    "        # 你可以根据具体需求修改损失函数\n",
    "        loss = compute_loss(node_embeddings, data)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1:03d}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    " # 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph data...\n",
      "\n",
      "Graph summary:\n",
      "Number of nodes: 3769\n",
      "Number of edges: 12939\n",
      "\n",
      "Node types and counts:\n",
      "  center: 1\n",
      "  tenant: 23\n",
      "  network_element: 86\n",
      "  virtual_machine: 3034\n",
      "  ha: 6\n",
      "  host: 575\n",
      "  tru: 6\n",
      "  tor: 32\n",
      "  eor: 6\n",
      "Graph loaded: 3769 nodes, 12939 edges\n",
      "\n",
      "Verifying graph structure:\n",
      "Sample node attributes: ('xbxa_dc4', {'type': 'center', 'node_id': 0})\n",
      "Sample edge attributes: ('xbxa_dc4', 'gs_B5G', {'connection_type': 'physical'})\n"
     ]
    }
   ],
   "source": [
    "# 加载图数据\n",
    "# 使用函数\n",
    "print(\"Loading graph data...\")\n",
    "graph_data = load_graph_data()\n",
    "print(f\"Graph loaded: {graph_data.number_of_nodes()} nodes, {graph_data.number_of_edges()} edges\")\n",
    "\n",
    "# 验证图的完整性\n",
    "print(\"\\nVerifying graph structure:\")\n",
    "print(\"Sample node attributes:\", next(iter(graph_data.nodes(data=True))))\n",
    "print(\"Sample edge attributes:\", next(iter(graph_data.edges(data=True))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset...\n",
      "Number of node types: 9\n"
     ]
    }
   ],
   "source": [
    "# 创建数据集\n",
    "print(\"Creating dataset...\")\n",
    "dataset = ResourceDataset(graph_data)\n",
    "print(f\"Number of node types: {len(dataset.node_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "Starting training...\n",
      "Epoch 010, Loss: 0.3208\n",
      "Epoch 020, Loss: 0.2843\n",
      "Epoch 030, Loss: 0.2514\n",
      "Epoch 040, Loss: 0.2323\n",
      "Epoch 050, Loss: 0.2045\n",
      "Epoch 060, Loss: 0.1882\n",
      "Epoch 070, Loss: 0.1765\n",
      "Epoch 080, Loss: 0.1465\n",
      "Epoch 090, Loss: 0.1315\n",
      "Epoch 100, Loss: 0.1273\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "print(\"Creating model...\")\n",
    "model = ResourceGNN(\n",
    "    num_node_features=len(dataset.node_types),\n",
    "    hidden_dim=64\n",
    ").to(device)\n",
    "\n",
    "# 训练模型\n",
    "print(\"Starting training...\")\n",
    "model = train_model(model, dataset, device, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kk_gnn_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
